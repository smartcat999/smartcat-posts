<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Smartcat&#39;s Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="[TOC]
1. development 1.1 requirement goland / vscode kt connect / telepresence 1.2 install download
$ git clone https://github.com/kubesphere/kubesphere.git$ cd kubesphere update config dir
# kubesphere/pkg/apiserver/config/config.go# replace defaultConfigurationPath = &#34;/etc/kubesphere&#34; =&gt; ${customer_dir} # kubesphere/pkg/models/routers/routers.go# replace ingressControllerFolder = &#34;/etc/kubesphere/ingress-controller&#34; =&gt; ${customer_ingress_dir} download yaml
# 通过ks管理页面/kubectl获取ks-apiserver的配置文件的config-mapkubesphere-config# kubectl.exe get cm/kubesphere-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml# kubesphere.yaml 放在步骤2中的 ${customer_dir} 目录下ks-router-config# kubectl.exe get cm/ks-router-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml# ingress-controller-svc.">
    <meta name="generator" content="Hugo 0.104.3" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="" />
<meta property="og:description" content="[TOC]
1. development 1.1 requirement goland / vscode kt connect / telepresence 1.2 install download
$ git clone https://github.com/kubesphere/kubesphere.git$ cd kubesphere update config dir
# kubesphere/pkg/apiserver/config/config.go# replace defaultConfigurationPath = &#34;/etc/kubesphere&#34; =&gt; ${customer_dir} # kubesphere/pkg/models/routers/routers.go# replace ingressControllerFolder = &#34;/etc/kubesphere/ingress-controller&#34; =&gt; ${customer_ingress_dir} download yaml
# 通过ks管理页面/kubectl获取ks-apiserver的配置文件的config-mapkubesphere-config# kubectl.exe get cm/kubesphere-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml# kubesphere.yaml 放在步骤2中的 ${customer_dir} 目录下ks-router-config# kubectl.exe get cm/ks-router-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml# ingress-controller-svc." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/kubesphere/" /><meta property="article:section" content="posts" />



<meta itemprop="name" content="">
<meta itemprop="description" content="[TOC]
1. development 1.1 requirement goland / vscode kt connect / telepresence 1.2 install download
$ git clone https://github.com/kubesphere/kubesphere.git$ cd kubesphere update config dir
# kubesphere/pkg/apiserver/config/config.go# replace defaultConfigurationPath = &#34;/etc/kubesphere&#34; =&gt; ${customer_dir} # kubesphere/pkg/models/routers/routers.go# replace ingressControllerFolder = &#34;/etc/kubesphere/ingress-controller&#34; =&gt; ${customer_ingress_dir} download yaml
# 通过ks管理页面/kubectl获取ks-apiserver的配置文件的config-mapkubesphere-config# kubectl.exe get cm/kubesphere-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml# kubesphere.yaml 放在步骤2中的 ${customer_dir} 目录下ks-router-config# kubectl.exe get cm/ks-router-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml# ingress-controller-svc.">

<meta itemprop="wordCount" content="1810">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="[TOC]
1. development 1.1 requirement goland / vscode kt connect / telepresence 1.2 install download
$ git clone https://github.com/kubesphere/kubesphere.git$ cd kubesphere update config dir
# kubesphere/pkg/apiserver/config/config.go# replace defaultConfigurationPath = &#34;/etc/kubesphere&#34; =&gt; ${customer_dir} # kubesphere/pkg/models/routers/routers.go# replace ingressControllerFolder = &#34;/etc/kubesphere/ingress-controller&#34; =&gt; ${customer_ingress_dir} download yaml
# 通过ks管理页面/kubectl获取ks-apiserver的配置文件的config-mapkubesphere-config# kubectl.exe get cm/kubesphere-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml# kubesphere.yaml 放在步骤2中的 ${customer_dir} 目录下ks-router-config# kubectl.exe get cm/ks-router-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml# ingress-controller-svc."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Smartcat&#39;s Blog
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1"></h1>
      
      
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>[TOC]</p>
<h4 id="1-development">1. development</h4>
<h5 id="11-requirement">1.1 requirement</h5>
<ul>
<li><strong>goland</strong> / <strong>vscode</strong></li>
<li><strong>kt connect</strong> / <strong>telepresence</strong></li>
</ul>
<h5 id="12-install">1.2 install</h5>
<ol>
<li>
<p>download</p>
<pre tabindex="0"><code>$ git clone https://github.com/kubesphere/kubesphere.git
$ cd kubesphere 
</code></pre></li>
<li>
<p>update config dir</p>
<pre tabindex="0"><code># kubesphere/pkg/apiserver/config/config.go
# replace defaultConfigurationPath = &#34;/etc/kubesphere&#34; =&gt; ${customer_dir}
</code></pre><pre tabindex="0"><code># kubesphere/pkg/models/routers/routers.go
# replace ingressControllerFolder = &#34;/etc/kubesphere/ingress-controller&#34; =&gt; ${customer_ingress_dir}
</code></pre></li>
<li>
<p>download yaml</p>
<pre tabindex="0"><code># 通过ks管理页面/kubectl获取ks-apiserver的配置文件的config-map
kubesphere-config
# kubectl.exe get cm/kubesphere-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml
# kubesphere.yaml 放在步骤2中的 ${customer_dir} 目录下

ks-router-config
# kubectl.exe get cm/ks-router-config -n kubesphere-system --kubeconfig=.kube\config-73-205 -o yaml
# ingress-controller-svc.yaml &amp;&amp; ingress-controller.yaml 放在步骤2中的 ${customer_ingress_dir8} 目录下
</code></pre></li>
<li>
<p>run &amp;&amp; server exchange</p>
<pre tabindex="0"><code>$ go run kubesphere.io/kubesphere/cmd/ks-apiserver --kubeconfig=C:/Users/wupeng/.kube/config-73-205 
$ ktctl.exe exchange ks-apiserver --expose 9090:9090 -n kubesphere-system -c .\.kube\config-73-205

# 修改token过期时间
# 修改kubesphere.yaml: authentication.oauthOptions.accessTokenMaxAge = 0
</code></pre><pre tabindex="0"><code>$ ktctl.exe recover ks-apiserver -n kubesphere-system -c .\.kube\config-73-205
$ ktctl.exe clean -c .\.kube\config-73-205
</code></pre></li>
<li>
<p>local debug
<a href="https://kubesphere.com.cn/forum/d/7891-kubesphere/17">cookie识别异常</a></p>
<pre tabindex="0"><code># 权限异常：设置 header: Authorization: Bearer ${token}
# token为header: cookie中的token信息
</code></pre></li>
</ol>
<h4 id="2--crd">2  crd</h4>
<h5 id="21-概念">2.1 概念</h5>
<p><strong>本质上k8s内置资源也是一种crd</strong>，由k8s官方实现了内置资源的生命周期管理</p>
<table>
<thead>
<tr>
<th>pod</th>
<th>crd</th>
</tr>
</thead>
<tbody>
<tr>
<td>nginx</td>
<td>nginx-config-cr</td>
</tr>
<tr>
<td>kong</td>
<td>kong-config-cr</td>
</tr>
</tbody>
</table>
<h5 id="22-生命周期交互流程">2.2 生命周期交互流程</h5>
<pre tabindex="0"><code class="language-puml" data-lang="puml">@startuml
actor client
control KubeApiServer as Api
database ETCD

==资源状态crud==
client -&gt; Api: 创建/查询/更新/删除资源
Api -&gt; ETCD
ETCD --&gt; client
==资源状态/事件监听==
Api -&gt; ETCD
Api -&gt; client: HTTP长连接+chunked / ws 事件推送
@enduml
</code></pre><h6 id="221-模式1">2.2.1 模式1</h6>
<p>一种资源 对应 一个模型
一个实例化资源对象 对应 一条实体对象记录</p>
<p>比如: 建立一个水果类型的crd资源，接着实例化一个苹果、香蕉、梨子等等</p>
<ol>
<li>用来做少量数据的存储、类似config-map/secret的作用</li>
<li>轻量级的api调用</li>
</ol>
<h6 id="222-模式2">2.2.2 模式2</h6>
<p>状态存储 + 事件推送</p>
<ol>
<li>异步/耗时事件处理
比如：订单状态变更处理/网关配置更新</li>
</ol>
<h5 id="23-crd的k8s状态管理组件-controller--informer">2.3 crd的k8s状态管理组件 controller / Informer</h5>
<p>[https://github.com/kubernetes/client-go/tree/master/tools/cache][源码实现]</p>
<ol>
<li>Reflector 状态监听</li>
<li>Queue 事件队列</li>
<li>Store 资源缓存</li>
<li>indexer/lister 资源索引/查询</li>
<li>WorkerQueue/Worker 任务队列/工作单元</li>
</ol>
<pre tabindex="0"><code class="language-puml" data-lang="puml">@startuml
control KubeApiServer as Api
boundary Reflector as ref
queue  Queue as queue
database Store as store
collections indexer as indexer
queue WorkerQueue as wq
actor handler as handler

Api-&gt;ref: 事件/推送
ref-&gt;queue: 事件预处理/过滤/分发/hook
ref-&gt;store: 资源缓存同步/更新
store--&gt;Api: 资源全量更新(初始化/周期性)
store-&gt;indexer: 构造索引/查询方式
queue-&gt;wq: obj name/namespaces
alt implement
    wq-&gt;handler: hook
    handler--&gt;indexer: query
end
@enduml
</code></pre><h5 id="24-aq">2.4 A&amp;Q</h5>
<h6 id="241-reflector实现原理">2.4.1 Reflector实现原理</h6>
<pre tabindex="0"><code>type controller struct {
	config         Config
	reflector      *Reflector
	reflectorMutex sync.RWMutex
	clock          clock.Clock
}

type Controller interface {
	// Run does two things.  One is to construct and run a Reflector
	// to pump objects/notifications from the Config&#39;s ListerWatcher
	// to the Config&#39;s Queue and possibly invoke the occasional Resync
	// on that Queue.  The other is to repeatedly Pop from the Queue
	// and process with the Config&#39;s ProcessFunc.  Both of these
	// continue until `stopCh` is closed.
	Run(stopCh &lt;-chan struct{})

	// HasSynced delegates to the Config&#39;s Queue
	HasSynced() bool

	// LastSyncResourceVersion delegates to the Reflector when there
	// is one, otherwise returns the empty string
	LastSyncResourceVersion() string
}

func (c *controller) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash()
	go func() {
		&lt;-stopCh
		c.config.Queue.Close()
	}()
	r := NewReflector(
		c.config.ListerWatcher,
		c.config.ObjectType,
		c.config.Queue,
		c.config.FullResyncPeriod,
	)
	r.ShouldResync = c.config.ShouldResync
	r.WatchListPageSize = c.config.WatchListPageSize
	r.clock = c.clock
	if c.config.WatchErrorHandler != nil {
		r.watchErrorHandler = c.config.WatchErrorHandler
	}

	c.reflectorMutex.Lock()
	c.reflector = r
	c.reflectorMutex.Unlock()

	var wg wait.Group

	wg.StartWithChannel(stopCh, r.Run) 

	wait.Until(c.processLoop, time.Second, stopCh) 
	wg.Wait()
}
</code></pre><pre tabindex="0"><code>// resource ListAndWatch
func (r *Reflector) Run(stopCh &lt;-chan struct{}) {
	klog.V(3).Infof(&#34;Starting reflector %s (%s) from %s&#34;, r.expectedTypeName, r.resyncPeriod, r.name)
	wait.BackoffUntil(func() {
		if err := r.ListAndWatch(stopCh); err != nil {
			r.watchErrorHandler(r, err)
		}
	}, r.backoffManager, true, stopCh)
	klog.V(3).Infof(&#34;Stopping reflector %s (%s) from %s&#34;, r.expectedTypeName, r.resyncPeriod, r.name)
}
</code></pre><pre tabindex="0"><code>// obj handler loop
func (c *controller) processLoop() {
	for {
		obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process))
		if err != nil {
			if err == ErrFIFOClosed {
				return
			}
			if c.config.RetryOnError {
				// This is the safe way to re-enqueue.
				c.config.Queue.AddIfNotPresent(obj)
			}
		}
	}
}
</code></pre><pre tabindex="0"><code>func (r *Reflector) ListAndWatch(stopCh &lt;-chan struct{}) error {
	klog.V(3).Infof(&#34;Listing and watching %v from %s&#34;, r.expectedTypeName, r.name)
	var resourceVersion string

	options := metav1.ListOptions{ResourceVersion: r.relistResourceVersion()}

	if err := func() error {
		go func() {
			defer func() {
				if r := recover(); r != nil {
					panicCh &lt;- r
				}
			}()
			
			pager := pager.New(pager.SimplePageFunc(func(opts metav1.ListOptions) (runtime.Object, error) {
				return r.listerWatcher.List(opts)
			}))
			list, paginatedResult, err = pager.List(context.Background(), options)
			close(listCh)
		}()
		select {
		case &lt;-stopCh:
			return nil
		case r := &lt;-panicCh:
			panic(r)
		case &lt;-listCh:
		}

	
		listMetaInterface, err := meta.ListAccessor(list)
		if err != nil {
			return fmt.Errorf(&#34;unable to understand list result %#v: %v&#34;, list, err)
		}
		resourceVersion = listMetaInterface.GetResourceVersion()
		
		items, err := meta.ExtractList(list)
		if err != nil {
			return fmt.Errorf(&#34;unable to understand list result %#v (%v)&#34;, list, err)
		}
		
		if err := r.syncWith(items, resourceVersion); err != nil {
			return fmt.Errorf(&#34;unable to sync list result: %v&#34;, err)
		}
	
		return nil
	}(); err != nil {
		return err
	}

	resyncerrc := make(chan error, 1)
	cancelCh := make(chan struct{})
	defer close(cancelCh)
	go func() {
		resyncCh, cleanup := r.resyncChan()
		defer func() {
			cleanup() // Call the last one written into cleanup
		}()
		for {
			select {
			case &lt;-resyncCh:
			case &lt;-stopCh:
				return
			case &lt;-cancelCh:
				return
			}
			if r.ShouldResync == nil || r.ShouldResync() {
				klog.V(4).Infof(&#34;%s: forcing resync&#34;, r.name)
				if err := r.store.Resync(); err != nil {
					resyncerrc &lt;- err
					return
				}
			}
			cleanup()
			resyncCh, cleanup = r.resyncChan()
		}
	}()

	for {
		// give the stopCh a chance to stop the loop, even in case of continue statements further down on errors
		select {
		case &lt;-stopCh:
			return nil
		default:
		}

		timeoutSeconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0))
		options = metav1.ListOptions{
			ResourceVersion: resourceVersion,
			TimeoutSeconds: &amp;timeoutSeconds,
			AllowWatchBookmarks: true,
		}

		// start the clock before sending the request, since some proxies won&#39;t flush headers until after the first watch event is sent
		start := r.clock.Now()
		w, err := r.listerWatcher.Watch(options)
		if err != nil {
			return err
		}

		if err := r.watchHandler(start, w, &amp;resourceVersion, resyncerrc, stopCh); err != nil {
		
		}
	}
}
</code></pre><pre tabindex="0"><code>func NewFilteredListWatchFromClient(c Getter, resource string, namespace string, optionsModifier func(options *metav1.ListOptions)) *ListWatch {
	listFunc := func(options metav1.ListOptions) (runtime.Object, error) {
		optionsModifier(&amp;options)
		return c.Get().
			Namespace(namespace).
			Resource(resource).
			VersionedParams(&amp;options, metav1.ParameterCodec).
			Do(context.TODO()).
			Get()
	}
	watchFunc := func(options metav1.ListOptions) (watch.Interface, error) {
		options.Watch = true
		optionsModifier(&amp;options)
		return c.Get().
			Namespace(namespace).
			Resource(resource).
			VersionedParams(&amp;options, metav1.ParameterCodec).
			Watch(context.TODO())
	}
	return &amp;ListWatch{ListFunc: listFunc, WatchFunc: watchFunc}
</code></pre><pre tabindex="0"><code>func (r *Reflector) watchHandler(start time.Time, w watch.Interface, resourceVersion *string, errc chan error, stopCh &lt;-chan struct{}) error {
	eventCount := 0
	defer w.Stop()

loop:
	for {
		select {
		case &lt;-stopCh:
			return errorStopRequested
		case err := &lt;-errc:
			return err
		case event, ok := &lt;-w.ResultChan():
			if !ok {
				break loop
			}
			if event.Type == watch.Error {
				return apierrors.FromObject(event.Object)
			}
			if r.expectedType != nil {
				if e, a := r.expectedType, reflect.TypeOf(event.Object); e != a {
					utilruntime.HandleError(fmt.Errorf(&#34;%s: expected type %v, but watch event object had type %v&#34;, r.name, e, a))
					continue
				}
			}
			if r.expectedGVK != nil {
				if e, a := *r.expectedGVK, event.Object.GetObjectKind().GroupVersionKind(); e != a {
					utilruntime.HandleError(fmt.Errorf(&#34;%s: expected gvk %v, but watch event object had gvk %v&#34;, r.name, e, a))
					continue
				}
			}
			meta, err := meta.Accessor(event.Object)
			if err != nil {
				utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to understand watch event %#v&#34;, r.name, event))
				continue
			}
			newResourceVersion := meta.GetResourceVersion()
			switch event.Type {
			case watch.Added:
				err := r.store.Add(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to add watch event object (%#v) to store: %v&#34;, r.name, event.Object, err))
				}
			case watch.Modified:
				err := r.store.Update(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to update watch event object (%#v) to store: %v&#34;, r.name, event.Object, err))
				}
			case watch.Deleted:
				err := r.store.Delete(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to delete watch event object (%#v) from store: %v&#34;, r.name, event.Object, err))
				}
			case watch.Bookmark:
				// A `Bookmark` means watch has synced here, just update the resourceVersion
			default:
				utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to understand watch event %#v&#34;, r.name, event))
			}
			*resourceVersion = newResourceVersion
			r.setLastSyncResourceVersion(newResourceVersion)
			if rvu, ok := r.store.(ResourceVersionUpdater); ok {
				rvu.UpdateResourceVersion(newResourceVersion)
			}
			eventCount++
		}
	}

	return nil
}
</code></pre><pre tabindex="0"><code>// Reflector watches a specified resource and causes all changes to be reflected in the given store.
type Reflector struct {
	// name identifies this reflector. By default it will be a file:line if possible.
	name string
	expectedTypeName string
	expectedType reflect.Type
	// The GVK of the object we expect to place in the store if unstructured.
	expectedGVK *schema.GroupVersionKind
	// The destination to sync up with the watch source
	store Store
	// listerWatcher is used to perform lists and watches.
	listerWatcher ListerWatcher

	// backoff manages backoff of ListWatch
	backoffManager wait.BackoffManager
	// initConnBackoffManager manages backoff the initial connection with the Watch calll of ListAndWatch.
	initConnBackoffManager wait.BackoffManager

	resyncPeriod time.Duration
	// ShouldResync is invoked periodically and whenever it returns `true` the Store&#39;s Resync operation is invoked
	ShouldResync func() bool
	// clock allows tests to manipulate time
	clock clock.Clock
	// paginatedResult defines whether pagination should be forced for list calls.
	// It is set based on the result of the initial list call.
	paginatedResult bool
	lastSyncResourceVersion string
	isLastSyncResourceVersionUnavailable bool
	// lastSyncResourceVersionMutex guards read/write access to lastSyncResourceVersion
	lastSyncResourceVersionMutex sync.RWMutex
	WatchListPageSize int64
	// Called whenever the ListAndWatch drops the connection with an error.
	watchErrorHandler WatchErrorHandler
}
</code></pre><h6 id="242-store同步机制">2.4.2 Store同步机制</h6>
<pre tabindex="0"><code>// 周期性同步
go func() {
		resyncCh, cleanup := r.resyncChan()
		defer func() {
			cleanup() // Call the last one written into cleanup
		}()
		for {
			select {
			case &lt;-resyncCh:
			case &lt;-stopCh:
				return
			case &lt;-cancelCh:
				return
			}
			if r.ShouldResync == nil || r.ShouldResync() {
				klog.V(4).Infof(&#34;%s: forcing resync&#34;, r.name)
				if err := r.store.Resync(); err != nil {
					resyncerrc &lt;- err
					return
				}
			}
			cleanup()
			resyncCh, cleanup = r.resyncChan()
		}
	}()
</code></pre><pre tabindex="0"><code>// 事件触发时同步
func (r *Reflector) watchHandler(start time.Time, w watch.Interface, resourceVersion *string, errc chan error, stopCh &lt;-chan struct{}) error {
	eventCount := 0
	defer w.Stop()

loop:
	for {
		select {
		case &lt;-stopCh:
			return errorStopRequested
		case err := &lt;-errc:
			return err
		case event, ok := &lt;-w.ResultChan():
			meta, err := meta.Accessor(event.Object)
			if err != nil {
				utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to understand watch event %#v&#34;, r.name, event))
				continue
			}
			newResourceVersion := meta.GetResourceVersion()
			switch event.Type {
			case watch.Added:
				err := r.store.Add(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to add watch event object (%#v) to store: %v&#34;, r.name, event.Object, err))
				}
			case watch.Modified:
				err := r.store.Update(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to update watch event object (%#v) to store: %v&#34;, r.name, event.Object, err))
				}
			case watch.Deleted:
				err := r.store.Delete(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to delete watch event object (%#v) from store: %v&#34;, r.name, event.Object, err))
				}
			case watch.Bookmark:
				// A `Bookmark` means watch has synced here, just update the resourceVersion
			default:
				utilruntime.HandleError(fmt.Errorf(&#34;%s: unable to understand watch event %#v&#34;, r.name, event))
			}
			*resourceVersion = newResourceVersion
			r.setLastSyncResourceVersion(newResourceVersion)
			if rvu, ok := r.store.(ResourceVersionUpdater); ok {
				rvu.UpdateResourceVersion(newResourceVersion)
			}
			eventCount++
		}
	}

	return nil
}
</code></pre><h6 id="243-完全使用k8s-cache机制带来的影响">2.4.3 完全使用k8s cache机制带来的影响</h6>
<ol>
<li>缓存的资源功能性设计/数据量上限制</li>
<li>对于资源数据的查询只支持简单的过滤查询以及query的过滤需要</li>
</ol>
<h6 id="244-缓存同步如果有很多个informer全量同步对性能损耗很大">2.4.4 缓存同步，如果有很多个informer全量同步，对性能损耗很大</h6>
<ol>
<li>全量缓存</li>
<li>增量缓存</li>
<li>bookmark机制</li>
</ol>
<h6 id="245-多个事件并发event-对于event的处理">2.4.5 多个事件并发event, 对于event的处理</h6>
<ol>
<li>DeltaFIEOQueue事件合并</li>
</ol>
<h5 id="25-client-go">2.5 client-go</h5>
<h6 id="251-架构图">2.5.1 架构图</h6>
<p><img src="img.png" alt=""></p>
<h6 id="252-guide">2.5.2 guide</h6>
<ol>
<li>
<p>quick start</p>
<pre tabindex="0"><code>$ go get k8s.io/client-go@latest
</code></pre></li>
<li>
<p>example</p>
<pre tabindex="0"><code>package main

import (
	&#34;context&#34;
	&#34;fmt&#34;
	v1 &#34;k8s.io/api/apps/v1&#34;
	core_v1 &#34;k8s.io/api/core/v1&#34;
	metav1 &#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34;
	&#34;k8s.io/apimachinery/pkg/runtime/schema&#34;
	_ &#34;k8s.io/client-go&#34;
	&#34;k8s.io/client-go/kubernetes&#34;
	&#34;k8s.io/client-go/kubernetes/scheme&#34;
	&#34;k8s.io/client-go/rest&#34;
	&#34;k8s.io/client-go/tools/clientcmd&#34;
	&#34;k8s.io/client-go/util/homedir&#34;
	&#34;path/filepath&#34;
)

func main() {
	//lib.ClientGoDemo01()
	config, err := clientcmd.BuildConfigFromFlags(&#34;&#34;, filepath.Join(homedir.HomeDir(), &#34;.kube/config-73-205&#34;))
	if err != nil {
		panic(err)
	}
	config.GroupVersion = &amp;schema.GroupVersion{Group: &#34;apps&#34;, Version: &#34;v1&#34;}
	config.NegotiatedSerializer = scheme.Codecs
	config.APIPath = &#34;/apis&#34;
	restclient, err := rest.RESTClientFor(config)
	if err != nil {
		panic(err)
	}

	deploy := v1.Deployment{}
	if err = restclient.Get().Namespace(&#34;kubesphere-system&#34;).
		Resource(&#34;deployments&#34;).
		Name(&#34;ks-apiserver&#34;).
		Do(context.Background()).Into(&amp;deploy); err != nil {
		panic(err)
	}
	fmt.Println(deploy)

	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		panic(err)
	}
	pod := &amp;core_v1.Pod{}
	if pod, err = clientset.CoreV1().Pods(&#34;kubesphere-system&#34;).Get(context.Background(),
		&#34;ks-apiserver-7c8c448bbb-xk7bx&#34;, metav1.GetOptions{}); err != nil {
		panic(err)
	}
	fmt.Println(*pod)
}
</code></pre></li>
</ol>
<h4 id="3-kubesphere">3 kubesphere</h4>
<h5 id="31-项目结构">3.1 项目结构</h5>
<ul>
<li>ks-console: 管理界面</li>
<li>ks-apiserver: API内部通信接口</li>
<li>ks-controller-manager: 所有CRD业务的Controller管理中心
<a href="https://www.onemodel.app/d/e8fUjiUmI0oOK2s1ITJEu">ks</a></li>
</ul>
<h5 id="32-登录流程">3.2 登录流程</h5>
<pre tabindex="0"><code class="language-puml" data-lang="puml">@startuml

actor User as user
participant Console as console
participant ks_apiserver as api
participant controller_manager as controller
participant informer
participant identity_provider

user-&gt;console: login
user-&gt;console: post /login params: username password(encrypt) 
console-&gt;api: post /oauth/token: username password grant_type
api-&gt;identity_provider: 根据grant_type选择认证方式
identity_provider--&gt;api: 返回identity信息
api-&gt;informer:校验identity关联的user cr信息
api--&gt;console:redirect / or /login/confirm or /password/confirm
console-&gt;console: redirect /dashboard

@enduml
</code></pre><h4 id="4-question">4 question</h4>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Smartcat's Blog 2022 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
