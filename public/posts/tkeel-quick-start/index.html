<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Smartcat&#39;s Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="[TOC]
1 install 1.1 k8s手动安装可能遇到的问题 虚拟机网卡启动失败
network-manager服务与network.service冲突；关闭network-manage服务并在systemd服务中禁用掉 安装速度太慢
docker 设置国内镜像源，或者使用代理，将需要的镜像提前下到本地 2 docker 2.1 images docker images # output: REPOSITORY TAG IMAGE ID CREATED SIZE tkeelio/rudder dev20220513 9e9725a9e275 3 days ago 61.8MB tkeelio/keel dev20220513 docker rmi {IMAGE ID} 2.2 container docker ps [-a] # output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 71b3272a6e55 openzipkin/zipkin &#34;start-zipkin&#34; 5 days ago Up 5 days (healthy) 9410/tcp, 0.0.0.0:9411-&gt;9411/tcp, :::9411-&gt;9411/tcp dapr_zipkin 9da93349655f daprio/dapr:1.7.1 &#34;./placement&#34; 5 days ago Up 5 days 0.">
    <meta name="generator" content="Hugo 0.104.3" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="" />
<meta property="og:description" content="[TOC]
1 install 1.1 k8s手动安装可能遇到的问题 虚拟机网卡启动失败
network-manager服务与network.service冲突；关闭network-manage服务并在systemd服务中禁用掉 安装速度太慢
docker 设置国内镜像源，或者使用代理，将需要的镜像提前下到本地 2 docker 2.1 images docker images # output: REPOSITORY TAG IMAGE ID CREATED SIZE tkeelio/rudder dev20220513 9e9725a9e275 3 days ago 61.8MB tkeelio/keel dev20220513 docker rmi {IMAGE ID} 2.2 container docker ps [-a] # output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 71b3272a6e55 openzipkin/zipkin &#34;start-zipkin&#34; 5 days ago Up 5 days (healthy) 9410/tcp, 0.0.0.0:9411-&gt;9411/tcp, :::9411-&gt;9411/tcp dapr_zipkin 9da93349655f daprio/dapr:1.7.1 &#34;./placement&#34; 5 days ago Up 5 days 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/tkeel-quick-start/" /><meta property="article:section" content="posts" />



<meta itemprop="name" content="">
<meta itemprop="description" content="[TOC]
1 install 1.1 k8s手动安装可能遇到的问题 虚拟机网卡启动失败
network-manager服务与network.service冲突；关闭network-manage服务并在systemd服务中禁用掉 安装速度太慢
docker 设置国内镜像源，或者使用代理，将需要的镜像提前下到本地 2 docker 2.1 images docker images # output: REPOSITORY TAG IMAGE ID CREATED SIZE tkeelio/rudder dev20220513 9e9725a9e275 3 days ago 61.8MB tkeelio/keel dev20220513 docker rmi {IMAGE ID} 2.2 container docker ps [-a] # output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 71b3272a6e55 openzipkin/zipkin &#34;start-zipkin&#34; 5 days ago Up 5 days (healthy) 9410/tcp, 0.0.0.0:9411-&gt;9411/tcp, :::9411-&gt;9411/tcp dapr_zipkin 9da93349655f daprio/dapr:1.7.1 &#34;./placement&#34; 5 days ago Up 5 days 0.">

<meta itemprop="wordCount" content="2161">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="[TOC]
1 install 1.1 k8s手动安装可能遇到的问题 虚拟机网卡启动失败
network-manager服务与network.service冲突；关闭network-manage服务并在systemd服务中禁用掉 安装速度太慢
docker 设置国内镜像源，或者使用代理，将需要的镜像提前下到本地 2 docker 2.1 images docker images # output: REPOSITORY TAG IMAGE ID CREATED SIZE tkeelio/rudder dev20220513 9e9725a9e275 3 days ago 61.8MB tkeelio/keel dev20220513 docker rmi {IMAGE ID} 2.2 container docker ps [-a] # output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 71b3272a6e55 openzipkin/zipkin &#34;start-zipkin&#34; 5 days ago Up 5 days (healthy) 9410/tcp, 0.0.0.0:9411-&gt;9411/tcp, :::9411-&gt;9411/tcp dapr_zipkin 9da93349655f daprio/dapr:1.7.1 &#34;./placement&#34; 5 days ago Up 5 days 0."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Smartcat&#39;s Blog
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1"></h1>
      
      
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>[TOC]</p>
<h4 id="1-install">1 install</h4>
<h5 id="11-k8s手动安装可能遇到的问题">1.1 k8s手动安装可能遇到的问题</h5>
<ol>
<li>虚拟机网卡启动失败<br>
<code>network-manager服务与network.service冲突；关闭network-manage服务并在systemd服务中禁用掉</code></li>
<li>安装速度太慢<br>
<code> docker 设置国内镜像源，或者使用代理，将需要的镜像提前下到本地</code></li>
</ol>
<h4 id="2-docker">2 docker</h4>
<h5 id="21-images">2.1 images</h5>
<ul>
<li>docker images
<pre tabindex="0"><code># output:
REPOSITORY                           TAG                  IMAGE ID       CREATED         SIZE
tkeelio/rudder                       dev20220513          9e9725a9e275   3 days ago      61.8MB
tkeelio/keel                         dev20220513
</code></pre></li>
<li>docker rmi {IMAGE ID}</li>
</ul>
<h5 id="22-container">2.2 container</h5>
<ul>
<li>docker ps [-a]
<pre tabindex="0"><code># output:
CONTAINER ID   IMAGE                  COMMAND                  CREATED      STATUS                PORTS                                                NAMES
71b3272a6e55   openzipkin/zipkin      &#34;start-zipkin&#34;           5 days ago   Up 5 days (healthy)   9410/tcp, 0.0.0.0:9411-&gt;9411/tcp, :::9411-&gt;9411/tcp  dapr_zipkin
9da93349655f   daprio/dapr:1.7.1      &#34;./placement&#34;            5 days ago   Up 5 days             0.0.0.0:50005-&gt;50005/tcp, :::50005-&gt;50005/tcp        dapr_placement
0603d14ee8f8   redis                  &#34;docker-entrypoint.s…&#34;   5 days ago   Up 5 days             0.0.0.0:6379-&gt;6379/tcp, :::6379-&gt;6379/tcp
</code></pre></li>
<li>docker rm {CONTAINER ID}</li>
</ul>
<h5 id="23-volume">2.3 volume</h5>
<h5 id="24-build">2.4 build</h5>
<ol>
<li>Dockerfile
<pre tabindex="0"><code>FROM alpine:3.13
ENV PLUGIN_ID=rudder
COPY ./rudder /
CMD [&#34;/rudder&#34;]
</code></pre></li>
<li>docker build<br>
<strong>docker build -t {IMAGE}:{IMAGE TAG} -f {Dockerfile} {Workspace}</strong>
<pre tabindex="0"><code># 在当前目录 ./ 下通过目录下的 Dockerfile 构建 tag 为 test/nginx:dev-20220516 的镜像到本地
# 构建完成后在本地可以通过 $ docker images 查看
$ docker build -t test/nginx:dev-20220516 -f ./Dockerfile .
</code></pre></li>
</ol>
<h5 id="25-containerd">2.5 containerd</h5>
<pre tabindex="0"><code>$ ctr -n k8s.io images ls
$ ctr -n k8s.io i import *.tar
$ ctr -n k8s.io i rm ImageID
</code></pre><h4 id="3-k8s">3 k8s</h4>
<h5 id="31-k8s集群访问kubectl">3.1 k8s集群访问（kubectl）</h5>
<h6 id="311-配置文件">3.1.1 配置文件</h6>
<ol>
<li>配置文件位置
<pre tabindex="0"><code># 用来访问集群的配置文件config位置
# 自行搭建的集群: /etc/kubernetes/admin.conf or ~/.kube/conf
# 用其他方式搭建的集群: ~/.kube/conf
$ kubectl config view
# output:
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:42177          
  name: kind-kind
contexts:
- context:
    cluster: kind-kind
    user: kind-kind
  name: kind-kind
current-context: kind-kind
kind: Config
preferences: {}
users:
- name: kind-kind
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
</code></pre></li>
<li>指定配置文件
<pre tabindex="0"><code>$ kubectl --kubeconfg=~/.kube/config-other cluster-info
</code></pre></li>
<li>设置环境变量（推荐使用）
<pre tabindex="0"><code># 推荐写入到 ~/.zshrc or ~/.bashrc
# kubectl 会自动将KUBECONFIG环境变量中的多个有效配置合并
$ export KUBECONFIG=~/.kube/config:~/.kube/config-123.9:~/.kube/config-qingcloud02:~/.kube/config-k5:~/.kube/config-home-vm02
# 激活环境变量
$ source ~/.zshrc or source ~/.bashrc  
# 后续可以直接使用kubectl
$ kubectl cluster-info
</code></pre></li>
</ol>
<h6 id="312-多集群配置管理">3.1.2 多集群配置管理</h6>
<ul>
<li>多集群配置切换
<pre tabindex="0"><code># 列出当前配置文件中可用的集群配置上下文
# 带 * 的代表当前使用的集群上下文
$ kubectl config get-contexts 
# output:
CURRENT   NAME                          CLUSTER          AUTHINFO           NAMESPACE
          docker-desktop                docker-desktop   docker-desktop
          kind-app01                    kind-app01       kind-app01
*         kind-kind                     kind-kind        kind-kind
          kubernetes-admin@kubernetes   kubernetes       kubernetes-admin
# 切换上下文
$ kubectl config use-context kubernetes-admin@kubernetes
# output:
Switched to context &#34;kubernetes-admin@kubernetes&#34;.
# 查看集群信息,获取控制节点信息
$ kubectl cluster-info
# output:
Kubernetes control plane is running at https://127.0.0.1:42177
CoreDNS is running at https://127.0.0.1:42177/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
# 获取集群所有节点信息
$ kubectl get nodes
# 单节点的k8s集群
# output:
NAME                 STATUS   ROLES                  AGE    VERSION
kind-control-plane   Ready    control-plane,master   7d3h   v1.23.0
# 获取集群节点详细信息
$ kubectl get nodes -o wide
# 多节点的k8s集群
# output:
NAME          STATUS   ROLES    AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
master1       Ready    master   232d   v1.19.8   192.168.123.9    &lt;none&gt;        Ubuntu 18.04.5 LTS   4.15.0-121-generic   docker://20.10.6
worker-s001   Ready    worker   232d   v1.19.8   192.168.123.11   &lt;none&gt;        Ubuntu 18.04.5 LTS   4.15.0-121-generic   docker://20.10.6
worker-s002   Ready    worker   232d   v1.19.8   192.168.123.12   &lt;none&gt;        Ubuntu 18.04.5 LTS   4.15.0-121-generic   docker://20.10.6
worker-s003   Ready    worker   89d    v1.19.8   192.168.123.3    &lt;none&gt;        Ubuntu 18.04.5 LTS   4.15.0-121-generic   docker://20.10.6
</code></pre></li>
</ul>
<h6 id="313-多集群配置冲突">3.1.3 多集群配置冲突</h6>
<ul>
<li>多集群默认配置中user/cluster/context使用kubernetes-admin/kubernetes/kubernetes-admin@kubernetes导致kubectl config切换失败</li>
</ul>
<pre tabindex="0"><code># 对每个配置文件中的user/cluster/context重命名使用不同的名称
</code></pre><ul>
<li>other</li>
</ul>
<h5 id="32-命名空间">3.2 命名空间</h5>
<ol>
<li>基本概念: Namespace是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。</li>
<li>查看命名空间
<pre tabindex="0"><code>$ kubectl get namespaces
# output:
NAME                              STATUS   AGE
default                           Active   7d4h
kube-node-lease                   Active   7d4h
kube-public                       Active   7d4h
kube-system                       Active   7d4h
kubesphere-controls-system        Active   2d23h
kubesphere-monitoring-federated   Active   2d23h
kubesphere-monitoring-system      Active   2d23h
kubesphere-system                 Active   2d23h
local-path-storage                Active   7d4h
testing                           Active   7d4h
</code></pre></li>
<li>查看某个命名空间下的资源(不加命名空间默认找default下面的)
<pre tabindex="0"><code># 查看所有命名空间下的pod
$ kubectl get pods --all-namespaces
# output:
NAMESPACE                      NAME                                                  READY   STATUS    RESTARTS           AGE
kube-system                    coredns-64897985d-cm7ct                               1/1     Running   0                  7d5h
kube-system                    coredns-64897985d-hvsvx                               1/1     Running
testing                        clickhouse-tkeel-core-s0-r0-0                         1/1     Running   0                  7d4h
testing                        console-plugin-admin-custom-config-78486bc8d8-zqr6l   2/2     Running
# 查看testing命名空间下的pod
$ kubectl get pods -n testing
# output:
NAMESPACE                      NAME 
testing                        clickhouse-tkeel-core-s0-r0-0                          1/1     Running   0                 7d4h
testing                        console-plugin-admin-custom-config-78486bc8d8-zqr6l    2/2     Running
</code></pre></li>
</ol>
<h5 id="33-内置资源">3.3 内置资源</h5>
<h6 id="331-pod">3.3.1 pod</h6>
<ol>
<li>基本概念: 基本调度单元，多个容器的集合
<pre tabindex="0"><code>$ kubectl explain pod
# output:
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   &lt;string&gt;  # schema API版本
   kind &lt;string&gt;          # 资源类型
   metadata     &lt;Object&gt;  # 定义的元信息
   spec &lt;Object&gt;          # 详细的配置描述
   status       &lt;Object&gt;  # 运行状态
</code></pre></li>
<li>查看pod信息
<pre tabindex="0"><code># 注意查看资源信息时必须加命名空间
$ kubectl get pod -n testing
# output:
NAME                                                              READY   STATUS             RESTARTS         AGE
clickhouse-tkeel-core-s0-r0-0                                     1/1     Running            0                7d4h
console-plugin-admin-custom-config-78486bc8d8-zqr6l               2/2     Running            0                6d
console-plugin-admin-plugins-9c85d4564-6j6bv                      2/2     Running            0                7d3h
$ kubectl describe pod clickhouse-tkeel-core-s0-r0-0  -n testing
# output:
Name:         clickhouse-tkeel-core-s0-r0-0
Namespace:    testing
Priority:     0
Node:         kind-control-plane/172.18.0.2
Start Time:   Mon, 09 May 2022 11:41:45 +0800
Labels:       app.kubernetes.io/instance=tkeel-core
Annotations:  clickhouse/config-checksum: 1029ef1cb57f6f09e68e54c33ff1e99b66975a2fae83015f43e0839c23383e70
Status:       Running
Controlled By:  StatefulSet/clickhouse-tkeel-core-s0-r0
Containers:
  clickhouse:
    Container ID:   containerd://54d2ebbdd918f2e71214d7fd0ab2f69adab1abd5f381255ff493e6aa41422eb9
    Image:          yandex/clickhouse-server:22.1.3.7
    Image ID:       docker.io/yandex/clickhouse-server@sha256:1cbf75aabe1e2cc9f62d1d9929c318a59ae552e2700e201db985b92a9bcabc6e
    Ports:          9000/TCP, 8123/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Mon, 09 May 2022 11:45:53 +0800
    Ready:          True
</code></pre></li>
<li>基础配置
<pre tabindex="0"><code>apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
</code></pre></li>
</ol>
<p>4.字段解析</p>
<pre tabindex="0"><code>1. Volume
2. RestartPoliy
- Always：只要退出就重启
- OnFailure：失败退出（exit code不等于0）时重启
- Never：只要退出就不再重启
3. env-环境变量
- hostname
- 容器和Pod的基本信息
- 集群中服务的信息
4. ImagePullPolicy
- Always：不管镜像是否存在都会进行一次拉取
- Never：不管镜像是否存在都不会进行拉取 
- IfNotPresent：只有镜像不存在时，才会进行镜像拉取，默认为IfNotPresent
5. resources
- spec.containers[].resources.limits.cpu：CPU上限，可以短暂超过，容器也不会被停止 
- spec.containers[].resources.limits.memory：内存上限，不可以超过；如果超过，容器可能会被停止或调度到其他资源充足的机器上
- spec.containers[].resources.requests.cpu：CPU请求，可以超过
- spec.containers[].resources.requests.memory：内存请求，可以超过；但如果超过，容器可能会在Node内存不足时清理
6. 探针
# 两种探针支持exec、tcp和httpGet方式三种方式探测容器状态
- LivenessProbe：探测应用是否处于健康状态，如果不健康则删除重建改容器
- ReadinessProbe：探测应用是否启动完成并且处于正常服务状态，如果不正常则更新容器的状态
7. init container
# Init Container在所有容器运行之前执行
8. 生命周期&amp;钩子
- postStart： 容器启动后执行，注意由于是异步执行，它无法保证一定在ENTRYPOINT之后运行。如果失败，容器会被杀死，并根据RestartPolicy决定是否重启
- preStop：容器停止前执行，常用于资源清理。如果失败，容器同样也会被杀死
# 钩子的回调函数支持两种方式：
# exec：在容器内执行命令
# httpGet：向指定URL发起GET请求
9. nodeSelector
# 指定该Pod只想运行在带有id=node1标签的Node上
11. todo
</code></pre><h6 id="332-service">3.3.2 service</h6>
<ol>
<li>基本概念
<pre tabindex="0"><code># 4层负载均衡TCP/UDP
# CluesterIP &amp;&amp; ExternalIP
# Label Selector
# Headless Services
# External Name
# IPVS
# endpoints
</code></pre></li>
</ol>
<ul>
<li>ClusterIP</li>
<li>NodePort</li>
<li>LoadBalancer</li>
</ul>
<ol start="2">
<li>查看service信息
<pre tabindex="0"><code>$ kubectl get svc -n testing
$ kubectl describe svc/app-service -n testing

# DNS
# _my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example
# my-svc.my-namespace.svc.cluster-domain.example
~ $ cat /etc/resolv.conf
# output:
# search ps.svc.cluster.local svc.cluster.local cluster.local ap2a.qingcloud.com
# nameserver 10.96.0.10
# options ndots:5

# endpoints
$ kubectl get endpoints -n testing
</code></pre></li>
<li>基础配置
<pre tabindex="0"><code>kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
</code></pre></li>
</ol>
<ul>
<li>1</li>
<li>2</li>
</ul>
<h6 id="333-deployment">3.3.3 deployment</h6>
<ol>
<li>基础概念
<pre tabindex="0"><code>用来管理pod，实现滚动升级和回滚以及应用的扩容和缩容
</code></pre></li>
<li>查看deployment信息
<pre tabindex="0"><code>$ kubectl get deploy -n testing
$ kubeclt get deploy/nginx -n testing -o wide
$ kubeclt get deploy/nginx -n testing -o yaml
$ kubeclt describe deploy/nginx -n testing
</code></pre></li>
<li>基础配置
<pre tabindex="0"><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80   
</code></pre></li>
</ol>
<ul>
<li>1 升级&amp;回滚
<pre tabindex="0"><code># 查看更新历史
$ kubectl rollout history deployment/nginx-deployment
# 回退到上一个版本
$ kubectl rollout undo deployment/nginx-deployment
# 回退到历史指定版本
$ kubectl rollout undo deployment/nginx-deployment --to-revision=1
# 通过设置.spec.revisonHistoryLimit项来指定deployment最多保留多少revison历史记录。默认的会保留所有的revision；如果将该项设置为0，Deployment就不允许回退了
</code></pre></li>
<li>2 pod 扩容
<pre tabindex="0"><code># 调整pod的副本数
$ kubectl scale deployment nginx-deployment --replicas 10
# 基于HPA的自动扩容，基于当前系统资源自动选择合适的副本数量
$ kubectl autoscale deployment nginx-deployment --min=3 --max=10 --cpu-percent=80
</code></pre></li>
</ul>
<h6 id="334-ingress--ingress-controller">3.3.4 ingress &amp;&amp; ingress-controller</h6>
<ol>
<li>基础概念</li>
<li>查看ingress信息</li>
<li>基础配置</li>
</ol>
<ul>
<li>1</li>
<li>2</li>
</ul>
<h6 id="335-pv--pvc">3.3.5 pv &amp;&amp; pvc</h6>
<ol>
<li>基础概念</li>
<li>查看ingress&amp;&amp;ingress-controller信息</li>
<li>基础配置</li>
</ol>
<ul>
<li>1</li>
<li>2</li>
</ul>
<h4 id="4-helm">4 helm</h4>
<h5 id="41-生命周期">4.1 生命周期</h5>
<ol>
<li>用户返回 helm install foo</li>
<li>Helm库调用安装API</li>
<li>在 crds/目录中的CRD会被安装</li>
<li>在一些验证之后，库会渲染foo模板</li>
<li>库准备执行pre-install钩子(将hook资源加载到Kubernetes中)</li>
<li>库按照权重对钩子排序(默认将权重指定为0)，然后在资源种类排序，最后按名称正序排列</li>
<li>库先加载最小权重的钩子(从负到正)</li>
<li>库会等到钩子是 &ldquo;Ready&quot;状态(CRD除外)</li>
<li>库将生成的资源加载到Kubernetes中。注意如果设置了&ndash;wait参数，库会等所有资源是ready状态， 且所有资源准备就绪后才会执行post-install钩子。</li>
<li>库执行post-install钩子(加载钩子资源)。</li>
<li>库会等到钩子是&quot;Ready&quot;状态</li>
<li>库会返回发布对象(和其他数据)给客户端</li>
<li>客户端退出</li>
</ol>
<h5 id="42-values">4.2 Values</h5>
<p>value值的优先级 由高到低</p>
<ul>
<li>使用&ndash;set (比如helm install &ndash;set foo=bar ./mychart)传递的单个参数</li>
<li>使用-f参数(helm install -f myvals.yaml ./mychart)传递到 helm install 或 helm upgrade的values文件</li>
<li>如果是子chart，就是父chart中的values.yaml文件</li>
<li>chart中的values.yaml文件</li>
</ul>
<h5 id="43-常用操作">4.3 常用操作</h5>
<pre tabindex="0"><code># 安装自定义的chart
$ helm install full-coral ./test_chart/

# 获取安装之后的chart清单
$ helm get manifest full-coral

# 卸载chart清单相关的资源
$ helm uninstall full-cora

# 展示渲染之后的模版，只渲染不安装
$ helm install --debug --dry-run goodly-guppy ./test_chart/
# output: 
install.go:178: [debug] Original chart version: &#34;&#34;
install.go:195: [debug] CHART PATH: /root/tmp/helm_test/test_chart

NAME: goodly-guppy
LAST DEPLOYED: Fri May 20 16:35:27 2022
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
{}

HOOKS:
MANIFEST:
---
# Source: test_chart/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: goodly-guppy-configmap
data:
  myvalue: &#34;Hello World&#34;
</code></pre><h5 id="44-helm插件开发">4.4 helm插件开发</h5>
<h6 id="441-步骤">4.4.1 步骤</h6>
<p><a href="https://docs.tkeel.io/developer_cookbook/tkeel_plugin/create">插件脚手架</a></p>
<h6 id="442-注意事项">4.4.2 注意事项</h6>
<ol>
<li>插件注解</li>
</ol>
<ul>
<li>tkeel插件注解</li>
</ul>
<pre tabindex="0"><code># 在tkeel中插件分为系统插件和自定义插件
# 自定义的插件需要在chart.yaml中按以下格式加入tkeel注解，否则在插件列表中默认会被系统过滤掉
- annotations:
   {
   tkeel.io/deployment-name: prometheus,
   tkeel.io/enable: &#34;true&#34;,
   tkeel.io/plugin-port: &#34;9202&#34;,
   tkeel.io/tag: prometheus,
   tkeel.io/version: v0.0.1
   }
</code></pre><ul>
<li>dapr应用注解
<a href="https://docs.dapr.io/operations/configuration/invoke-allowlist/">dapr配置文件</a></li>
</ul>
<pre tabindex="0"><code># template.metadata.annotations 中需要启用dapr相关配置
annotations:
  {
    &#34;dapr.io/enabled&#34;: &#34;true&#34;,
    &#34;dapr.io/app-id&#34;: prometheus,
    &#34;dapr.io/app-port&#34;: &#34;9202&#34;,
    &#34;dapr.io/dapr-grpc-port&#34;: &#34;50001&#34;,
    &#34;dapr.io/dapr-http-port&#34;: &#34;3500&#34;,
    &#34;dapr.io/metrics-port&#34;: &#34;9089&#34;,
    &#34;dapr.io/log-level&#34;: debug,
    &#34;dapr.io/enable-api-logging&#34;: &#34;true&#34;
  }

# 在插件pod启动后，会在annotations中插入dapr配置文件 dapr.io/config: prometheus
annotations:
    dapr.io/app-id: prometheus
    dapr.io/app-port: &#39;9202&#39;
    dapr.io/config: prometheus
    dapr.io/dapr-grpc-port: &#39;50001&#39;
    dapr.io/dapr-http-port: &#39;3500&#39;
    dapr.io/enable-api-logging: &#39;true&#39;
    dapr.io/enabled: &#39;true&#39;
    dapr.io/log-level: debug
    dapr.io/metrics-port: &#39;9089&#39;
    kubesphere.io/restartedAt: &#39;2022-06-06T08:33:19.415Z&#39;

# 该配置文件为自定义的CRD类型，可以在启动配置请求的访问权限/链路追踪的插件等等，详情见上述官网链接 
apiVersion: dapr.io/v1alpha1
kind: Configuration
metadata:
  annotations:
    meta.helm.sh/release-name: prometheus
    meta.helm.sh/release-namespace: testing
  labels:
    app.kubernetes.io/managed-by: Helm
  name: prometheus
  namespace: testing
spec:
  accessControl:
    defaultAction: deny
    policies:
      - appId: rudder
        defaultAction: allow
        namespace: testing
        trustDomain: tkeel
      - appId: keel
        defaultAction: allow
        namespace: testing
        trustDomain: tkeel
      - appId: rudder
        defaultAction: allow
        namespace: testing
        trustDomain: public   # 本地测试时设置为 public 避免 rudder 调用插件接口时 domain 不匹配 tkeel 导致 permission access deny
    trustDomain: tkeel
  httpPipeline:
    handlers:
      - name: prometheus-oauth2-client
        type: middleware.http.oauth2clientcredentials
  metric:
    enabled: true
</code></pre><ol start="2">
<li>other</li>
</ol>
<h4 id="5-middleware">5 middleware</h4>
<h5 id="51-redis">5.1 redis</h5>
<h6 id="511-install">5.1.1 install</h6>
<pre tabindex="0"><code>$ helm repo add bitnami https://charts.bitnami.com/bitnami
$ helm repo update
$ helm install redis bitnami/redis
# output: 
NAME: redis
LAST DEPLOYED: Thu May 19 14:58:47 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: redis
CHART VERSION: 16.9.6
APP VERSION: 6.2.7

** Please be patient while the chart is being deployed **

Redis&amp;trade; can be accessed on the following DNS names from within your cluster:

    redis-master.default.svc.cluster.local for read/write operations (port 6379)
    redis-replicas.default.svc.cluster.local for read-only operations (port 6379)



To get your password run:

    export REDIS_PASSWORD=$(kubectl get secret --namespace default redis -o jsonpath=&#34;{.data.redis-password}&#34; | base64 --decode)

To connect to your Redis&amp;trade; server:

1. Run a Redis&amp;trade; pod that you can use as a client:

   kubectl run --namespace default redis-client --restart=&#39;Never&#39;  --env REDIS_PASSWORD=$REDIS_PASSWORD  --image docker.io/bitnami/redis:6.2.7-debian-10-r19 --command -- sleep infinity

   Use the following command to attach to the pod:

   kubectl exec --tty -i redis-client --namespace default -- bash

2. Connect using the Redis&amp;trade; CLI:
   REDISCLI_AUTH=&#34;$REDIS_PASSWORD&#34; redis-cli -h redis-master
   REDISCLI_AUTH=&#34;$REDIS_PASSWORD&#34; redis-cli -h redis-replicas

To connect to your database from outside the cluster execute the following commands:

    kubectl port-forward --namespace default svc/redis-master 6379:6379 &amp;
    REDISCLI_AUTH=&#34;$REDIS_PASSWORD&#34; redis-cli -h 127.0.0.1 -p 6379
</code></pre><h6 id="512-access">5.1.2 access</h6>
<pre tabindex="0"><code>$ kubectl port-forward svc/tkeel-middleware-redis-master 6379:6379 -n testing
</code></pre><h4 id="6-tkeel">6 tkeel</h4>
<h5 id="61-install">6.1 install</h5>
<ul>
<li>logs</li>
</ul>
<pre tabindex="0"><code>$ kubectl get pods -n keel-system | grep rudder | awk &#39;{print $1}&#39; | head -n 1 | xargs -I {}  kubectl  logs {} -n keel-system --tail=10
$ kubectl get pods -n keel-system | grep keel | awk &#39;{print $1}&#39; | head -n 1 | xargs -I {}  kubectl logs {} -n keel-system --tail=10
</code></pre><h5 id="62-遇到的问题">6.2 遇到的问题</h5>
<ul>
<li>插件安装失败</li>
</ul>
<pre tabindex="0"><code>→  Deploying the tKeel Platform to your cluster...
↓  Deploying the tKeel Platform to your cluster... ℹ️  install tKeel middleware done.
↗  Deploying the tKeel Platform to your cluster... ℹ️  install tKeel platform  done.
→  Deploying the tKeel Platform to your cluster... W0509 11:41:40.516252   13571 warnings.go:70] policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
W0509 11:41:40.556146   13571 warnings.go:70] policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
↙  Deploying the tKeel Platform to your cluster... ℹ️  install tKeel component &lt;core&gt; done.
↑  Deploying the tKeel Platform to your cluster... ℹ️  install tKeel component &lt;rudder&gt; done.
✅  Deploying the tKeel Platform to your cluster...
http://127.0.0.1:39083/v1.0/invoke/rudder/method/v1/oauth2/admin?password=Y2hhbmdlbWU%3D
http://127.0.0.1:35137/v1.0/invoke/keel/method/apis/rudder/v1/repos/tkeel
http://127.0.0.1:43809/v1.0/invoke/keel/method/apis/rudder/v1/plugins/console-portal-admin
❌  Install &#34;console-portal-admin&#34; failed, Because: can&#39;t handle this
http://127.0.0.1:33349/v1.0/invoke/keel/method/apis/rudder/v1/plugins/console-portal-tenant
❌  Install &#34;console-portal-tenant&#34; failed, Because: can&#39;t handle this
http://127.0.0.1:43631/v1.0/invoke/keel/method/apis/rudder/v1/plugins/console-plugin-admin-plugins
❌  Install &#34;console-plugin-admin-plugins&#34; failed, Because: can&#39;t handle this
✅  Success! tKeel Platform has been installed to namespace testing. To verify, run `tkeel plugin list&#39; in your terminal. To get started, go here: https://tkeel.io/keel-getting-started
</code></pre><pre tabindex="0"><code># 解决方式
$ root@i-onaxtmf3:~# tkeel admin login -p changeme --print
# output:
http://127.0.0.1:37653/v1.0/invoke/rudder/method/v1/oauth2/admin?password=Y2hhbmdlbWU%3D
✅  You are Login Success!
✅  Your Token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJ0S2VlbCIsImV4cCI6IjIwMjItMDUtMDlUMDY6MjE6NTYuNDQwNDgwNDE4WiIsImlhdCI6IjIwMjItMDUtMDlUMDU6MjE6NTYuNDQwNDgwNDE4WiIsImlzcyI6InJ1ZGRlciIsImp0aSI6IjZkMjNkNjFjLWQwZDQtNDkyYy1iYmU3LTQzODYwN2Q1ODEwYiIsIm5iZiI6IjIwMjItMDUtMDlUMDU6MjE6NTYuNDQwNDgwNDE4WiIsInN1YiI6ImFkbWluIn0.MjKpJ4lQzbjfzeRiTPZki72zvqRpiZOFzOBt14PvMDo

# 添加仓库地址
$ tkeel repo add tkeel https://tkeel-io.github.io/helm-charts
$ tkeel repo add lunz https://lunz1207.github.io/helm-charts/

# 安装插件
$ echo &#34;console-portal-admin&#34; |  xargs -I {} tkeel plugin install repo-name/{}@0.5.0 {}
$ echo &#34;console-portal-tenant&#34; |  xargs -I {} tkeel plugin install repo-name/{}@0.5.0 {}
$ echo &#34;console-portal-admin&#34; |  xargs -I {} tkeel plugin install repo-name/{}@0.5.0 {}
</code></pre><ul>
<li>other</li>
<li></li>
</ul>
<h4 id="7-kubesphere">7 kubesphere</h4>
<h5 id="71-install">7.1 install</h5>
<h5 id="72-access">7.2 access</h5>
<pre tabindex="0"><code>#Console: http://172.18.0.2:30880
#Account: admin
#Password: P@88w0rd
#NewPassWord: Qwer9876

$ kubectl get pods -n kubesphere-system
$ kubectl get svc -n kubesphere-system
$ kubectl port-forward svc/ks-console 8888:80  -n kubesphere-system 
</code></pre><h4 id="8-tkeel">8 tkeel</h4>
<pre tabindex="0"><code># addons_identity
{
    &#34;res&#34;:
    {
        &#34;ret&#34;: 0,
        &#34;msg&#34;: &#34;ok&#34;
    },
    &#34;plugin_id&#34;: &#34;tkeel-device-template&#34;,
    &#34;version&#34;: &#34;v0.0.1&#34;,
    &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
    &#34;implemented_plugin&#34;:
    [
        {
            &#34;plugin&#34;:
            {
                &#34;id&#34;: &#34;tkeel-device&#34;,
                &#34;version&#34;: &#34;v0.4.1&#34;
            },
            &#34;addons&#34;:
            [
                {
                    &#34;addons_point&#34;: &#34;device-schema-change&#34;,
                    &#34;implemented_endpoint&#34;: &#34;ping&#34;
                }
            ]
        }
    ]
}
</code></pre><pre tabindex="0"><code># plugin_route
{
    &#34;&#34;:
    {
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-admin-custom-config&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-admin-custom-config&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-admin-notification-configs&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-admin-notification-configs&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-admin-plugins&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-admin-plugins&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-admin-service-monitoring&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-admin-service-monitoring&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-admin-tenants&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-admin-tenants&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-admin-usage-statistics&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-admin-usage-statistics&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-alarm-policy&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-alarm-policy&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-alarms&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-alarms&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-data-query&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-data-query&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-data-subscription&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-data-subscription&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-device-templates&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-device-templates&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-devices&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-devices&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-networks&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-networks&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-notification-objects&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-notification-objects&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-plugins&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-plugins&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-roles&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-roles&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-routing-rules&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-routing-rules&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-plugin-tenant-users&#34;:
    {
        &#34;id&#34;: &#34;console-plugin-tenant-users&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-portal-admin&#34;:
    {
        &#34;id&#34;: &#34;console-portal-admin&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;console-portal-tenant&#34;:
    {
        &#34;id&#34;: &#34;console-portal-tenant&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;core-broker&#34;:
    {
        &#34;id&#34;: &#34;core-broker&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;fluxswitch&#34;:
    {
        &#34;id&#34;: &#34;fluxswitch&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;iothub&#34;:
    {
        &#34;id&#34;: &#34;iothub&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;rule-manager&#34;:
    {
        &#34;id&#34;: &#34;rule-manager&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;tkeel-alarm&#34;:
    {
        &#34;id&#34;: &#34;tkeel-alarm&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;implemented_plugin&#34;:
        [
            &#34;tkeel-device&#34;
        ],
        &#34;version&#34;: &#34;1&#34;
    },
    &#34;tkeel-device&#34;:
    {
        &#34;id&#34;: &#34;tkeel-device&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;,
        &#34;register_addons&#34;:
        {
            &#34;device-schema-change&#34;: &#34;tkeel-alarm/v1/notify/update&#34;
        }
    },
    &#34;tkeel-monitor&#34;:
    {
        &#34;id&#34;: &#34;tkeel-monitor&#34;,
        &#34;status&#34;: 3,
        &#34;tkeel_version&#34;: &#34;v0.4.0&#34;,
        &#34;version&#34;: &#34;1&#34;
    }
}
</code></pre><ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Smartcat's Blog 2022 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
